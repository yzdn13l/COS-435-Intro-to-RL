{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2: Imitation Learning\n",
        "\n",
        "Welcome to the coding portion of Homework 2! This week's problem set focuses on implementing imitation learning/behavioral cloning. The goal is that by the end of the assignment, you will know how to implement behavioral cloning and evaluate the cloned policy. As an added bonus, you will get to train an agent to play Flappy Bird!  \n",
        "\n",
        "<center>\n",
        "<img width=\"300px\" src=\"https://drive.google.com/uc?id=1O4dAX_rN62fjsBRDjaYpz01ZcQ4aVP6A\">\n",
        "</center>\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "* **You should be running this on Google Colab!**\n",
        "* **Be sure to upload the dataset files provided with this .ipynb file using the folder tab on the left, so that you can have access to them.**\n"
      ],
      "metadata": {
        "id": "3v3EhFObtCJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: Always run this cell before anything else to ensure that you are able to access the Flappy Bird environment.\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%pip install git+https://github.com/kchua/flappy-bird-gym.git\n",
        "clear_output()\n",
        "import flappy_bird_gym\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "def animate_images(img_lst):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_axis_off()\n",
        "  ims = []\n",
        "  for i, image in enumerate(img_lst):\n",
        "    im = ax.imshow(image.swapaxes(0, 1), animated=True)\n",
        "    if i == 0:\n",
        "      ax.imshow(image.swapaxes(0, 1))\n",
        "    ims.append([im])\n",
        "  ani = animation.ArtistAnimation(fig, ims, interval=34)\n",
        "  return ani"
      ],
      "metadata": {
        "id": "X9nDr3QgSlqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Distribution"
      ],
      "metadata": {
        "id": "2piEzHQminlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b783f33b-b28a-4f02-c1be-ee9f88fe6ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Flap Like How Flappy Sr. Taught You! (AKA Implementing Behavioral Cloning)"
      ],
      "metadata": {
        "id": "aN3H3f1LcliY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Growing up, Flappy Jr. has always dreamed of being as graceful as his mentor, Flappy Sr., in navigating the randomly generated pipes of the world. However, one day, his mentor simply disappeared on a daily pipe dodging adventure -- eyewitness reports say that his mentor is still pipe dodging and has not reset since the day he left.\n",
        "\n",
        "One day, as he was going about his day, he found a secret cabinet in his mentor's house containing notes of all of his pipe dodging exploits. Flappy Jr. thought to himself, \"This is my chance! If I copy whatever he did in these notes, I will be as good as him.\"\n",
        "\n",
        "In the following problems, we will help Flappy Jr. by implementing behavioral cloning to make use of Flappy Sr.'s notes."
      ],
      "metadata": {
        "id": "O70GHQbvm76A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Policy Evaluation\n",
        "\n",
        "Before we get started with any training, let us make sure that we can evaluate whatever policy we get at the end. Recall that in reinforcement learning, the primary way in which we compare policies is by comparing their _returns_. The (infinite discounted) return of a policy $\\pi$, denoted $R(\\pi)$, is defined as\n",
        "\n",
        "$$R(\\pi) := \\mathbb{E}\\left[\\sum_{t = 0}^\\infty \\gamma^tr(s_t, a_t)\\right],$$\n",
        "\n",
        "where $s_0 \\sim p_0(s)$, and for all $t$, $a_t \\sim \\pi(s_t)$ and $s_{t + 1} \\sim p(s_{t + 1} \\ | \\ s_t, a_t)$, and $\\gamma$ is the discount factor."
      ],
      "metadata": {
        "id": "NrhEoUjWXuf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the following code box, write a function `evaluate_policy`, which given an environment `env`, a callable `policy` which returns an action distribution given a state, and discount factor `discount`, returns a single-sample estimate of $R(\\pi)$ as defined above.** `env` is assumed to follow the \\*old Gym API, and the action distribution returned by `policy` is a subclass of `torch.distributions.Distribution`. You can assume that the rollout eventually terminates.\n",
        "\n",
        "\\*In newer versions of Gym/Gymnasium, `reset` and `step` are assumed to return `(initial_ob, info_dict)` and `(next_ob, reward, terminated, truncated, info)`'. However, the environment we will be working with for this assignment instead returns `initial_ob` and `(next_ob, reward, done, info)`, following the old API.\n",
        "\n",
        "(An earlier version incorrectly had `init` instead of `reset` in the comment above, this has been fixed.)"
      ],
      "metadata": {
        "id": "DcGIvwdhNt7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(env, policy, discount) -> float:\n",
        "  \"\"\"Returns a single-sample estimate of a policy's return.\n",
        "\n",
        "  Args:\n",
        "    env: OpenAI gym environment following old API.\n",
        "    policy: Callable representing a policy.\n",
        "\n",
        "  Returns:\n",
        "    Single-sample estimate of return.\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE!\n",
        "  pass"
      ],
      "metadata": {
        "id": "FEhyB9RNXspl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As a test, use the function you defined in the block above to evaluate a policy which acts uniformly at random within the Flappy Bird environment over 50 independent rollouts. Use a discount of $0.999$.**\n",
        "\n",
        "(Note that the discount factor above was changed from $0.99$ in an earlier version; we will not be grading you based on which value you use, but $0.999$ will make it easier to compare different algorithms in this homework)."
      ],
      "metadata": {
        "id": "oXW-MPqITqwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
        "### YOUR CODE HERE!\n",
        "pass"
      ],
      "metadata": {
        "id": "qNy_TD0cTlUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Defining a Policy\n",
        "\n",
        "**In the code cell below, create a subclass of ``torch.nn.Module`` to represent a policy over a finite set of actions.** Remember, a policy outputs an instance of `torch.distributions.Distribution` (can be one of its subclasses), and the policy must be able to represent any distribution over the finite set of actions! **We have defined the skeleton for you below; do not modify the inputs to the functions.**\n",
        "\n",
        "Note: Feel free to define the architecture however you like; _all we need from you is that you properly initialize the module, and that the forward method is consistent with our definition of a policy._"
      ],
      "metadata": {
        "id": "INOeN-RmdOqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscretePolicy(nn.Module):\n",
        "  def __init__(self, input_dim, n_actions):\n",
        "    \"\"\"Initializes a policy over the action set {0, 1, ..., n_actions-1}.\n",
        "\n",
        "    Args:\n",
        "      input_dim: Observation dimensionality.\n",
        "      n_actions: Number of actions in environment.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    ### YOUR CODE HERE!\n",
        "    pass\n",
        "\n",
        "  def forward(self, ob) -> Distribution:\n",
        "    \"\"\"Returns a distribution over this policy's action set.\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE!\n",
        "    pass"
      ],
      "metadata": {
        "id": "wtTcQJVQiiEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Setting Up Behavioral Cloning\n",
        "\n",
        "For this problem, we will be defining a standard training loop to perform behavioral cloning. **In the following code cell, write a function which takes in:**\n",
        "\n",
        "* **a policy `policy` of class `DiscretePolicy` to be trained,**\n",
        "* **a dataset `dataset` for imitation learning,**\n",
        "* **number of training steps `n_steps`,**\n",
        "* **and batch size `batch_size`,**\n",
        "\n",
        "**and returns the resulting policy after performing training according to the given parameters. Please clearly specify the format in which you expect the dataset to take within the docstring.**\n",
        "\n"
      ],
      "metadata": {
        "id": "i5rzrXjpmLsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_policy_by_bc(policy, dataset, n_steps, batch_size) -> DiscretePolicy:\n",
        "  \"\"\"Trains the provided policy by behavioral cloning, by taking n_steps training steps with the\n",
        "  provided optimizer. During training, training batches of size batch_size are sampled from the dataset\n",
        "  to compute the loss.\n",
        "\n",
        "  Args:\n",
        "    policy: policy of class DiscretePolicy.\n",
        "    dataset: The dataset, represented as ____COMPLETE THIS!!!!!!!____.\n",
        "    n_steps: Number of training steps to take.\n",
        "    batch_size: Size of the sampled batch for each training step.\n",
        "\n",
        "  Returns:\n",
        "    A policy trained according to the parameters above.\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE!\n",
        "  pass"
      ],
      "metadata": {
        "id": "0rsDbZo4pa9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Behavioral Cloning and Evaluation\n",
        "\n",
        "The time has come for Flappy Jr. to learn from Flappy Sr.'s notes! **Make use of the provided dataset `flappy_sr_notes.mat` and obtain a policy for Flappy Jr. via behavioral cloning. Upon training, evaluate the policy using a discount of 0.99, and verify that it performs much better than selecting actions uniformly at random.** Remember to upload the provided dataset using the folder tab on the left to be able to access it!\n",
        "\n",
        "Hint 1: You can use `scipy.io.loadmat` to load `.mat` files. Note that uploading may take some time, and `loadmat()` will error out while this process is incomplete; you can check the progress of the upload at the bottom of the folder tab to the left.  \n",
        "Hint 2: Upon uploading the file, you can `right-click > Copy path` to get the path to the file within the Colab server.  \n",
        "Hint 3: `flappy_sr_notes.mat` contains a record of Flappy Sr.'s whole life: every one of his rollouts is contained within the dataset in sequential order. The keys available in the dataset are `observations`, `actions`, `rewards`, `next_observations`, and `is_rollout_start` (indicates whether each index is the start of a new rollout). You may not need all keys for behavioral cloning!"
      ],
      "metadata": {
        "id": "5PmL9w8Kp64T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "pass"
      ],
      "metadata": {
        "id": "ikJbbq_Dp50G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e) Optional (but Fun!): Visualizing Your Learned Policy Within the Game\n",
        "\n",
        "Want to see Flappy Jr. majestically soar through the skies? We sure do! We have written for you a convenience function for visualizing a sequence of images as an animation right here within the Colab environment. **All you need to do is to copy your sampling code here from Problem 1(a) and slightly modify it to instead return a sequence of rendered images of every timestep from a rollout.** The generated animation will be saved in the folder tab to the left where you uploaded the dataset.\n",
        "\n",
        "Hint: At any point, you can call `render(mode=\"rgb_array\")` on a `gym` environment to obtain an RGB image of the current state."
      ],
      "metadata": {
        "id": "cl--j0bEpm9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout_and_render(env, policy) -> list[np.ndarray]:\n",
        "  \"\"\"Returns a rendering of a single rollout under the provided policy.\n",
        "\n",
        "  Args:\n",
        "    env: OpenAI gym environment following old API.\n",
        "    policy: Callable representing a policy.\n",
        "\n",
        "  Returns:\n",
        "    A list of RGB images (as numpy arrays) from a single rollout.\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE!\n",
        "  pass\n",
        "\n",
        "\n",
        "img_lst = []    # Assign list of images here\n",
        "\n",
        "### DO NOT MODIFY ANYTHING BELOW THIS POINT\n",
        "ani = animate_images(img_lst)\n",
        "FFwriter = animation.FFMpegWriter(fps=30)\n",
        "ani.save('animation.mp4', writer=FFwriter)"
      ],
      "metadata": {
        "id": "WGPZYhugPBMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Floppy the Sloppy Ruins (?) the Day (AKA An Introduction to Filtered Behavioral Cloning)"
      ],
      "metadata": {
        "id": "Y-5BUGGbrWTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flappy Jr. happily went to sleep, satisfied that he has had such a succesful day of flying through the skies imitating Flappy Sr. Alas, he is woken up by some strange noise in the middle of the night, and immediately realizes that someone has run off with their learned model weights. Calmly, Flappy Jr. tells himself that Flappy Sr.'s notes are just in the next room, and that they can always do behavioral cloning again. Flappy Jr. walks into the next room and is horrified to see that the notebook being scribbled over by his nemesis, Floppy the Sloppy. Upon being discovered, Floppy the Sloppy flies off awkwardly flailing through the skies, laughing while they continue their awkward motion one could hardly consider as flying."
      ],
      "metadata": {
        "id": "nWvjAN6CrhuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a): What is Left???\n",
        "\n",
        "Unfortunately, Floppy the Sloppy has uncharacteristically good penmanship, and his cannot be distinguished from that of Flappy Sr. As a result, we now only have a vandalized dataset `vandalized_notes.mat` consisting of the combined trajectories of Floppy the Sloppy and Flappy Sr. which are indistinguishable from each other at first glance. **For this problem, try applying behavioral cloning to train a new policy as you have done before, only this time using `vandalized-notes.mat.** How does the performance compare to your policy from Problem 1(d)?"
      ],
      "metadata": {
        "id": "GaVPJw6kIaVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE!\n",
        "pass"
      ],
      "metadata": {
        "id": "oZuN-ylyKaUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: What does this experiment tell you about running behavioral cloning on noisy/low-quality datasets? Intuitively, why does this happen?**\n",
        "\n",
        "Hint: Think about what the BC loss is optimizing."
      ],
      "metadata": {
        "id": "Px-5Rzw_HAs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠"
      ],
      "metadata": {
        "id": "8TqihdoNHHoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b): Array of Hope???\n",
        "\n",
        "While deep in thought, Flappy Jr. had a sudden epiphany; he has access to reward data! He can then weigh the importance of imitating a particular example by looking at the quality of the outcome from performing a particular action.\n",
        "\n",
        "This strategy, where one reweighs BC examples by some predefined notion of quality, is generally referred to as **filtered behavioral cloning**. More formally, assume that we have access to a BC dataset $\\{(s_1, a_1), \\dots, (s_N, a_N)\\}$. Furthermore, assume that we have pre-defined weights $w_1, \\dots, w_N$ for each of the $N$ examples. Then, we can consider a _reweighed BC loss_\n",
        "\n",
        "$$L_w(\\pi) = \\frac{1}{N}\\sum_{i = 1}^{N}w_iL(\\pi(s_i), a_i),$$\n",
        "\n",
        "where $L$ is the standard per-example BC loss (e.g. cross-entropy loss for discrete actions). If the weights represent a notion of quality, one can interpret the loss above as performing filtering to ensure that only high quality examples are being imitated.\n",
        "\n",
        "**For this problem, you will fill in the skeleton below to implement a training loop based on filtered BC. Firstly, implement the reweighed BC loss given above. Secondly, modify the BC loop you implemented before to make use of this new loss function.**"
      ],
      "metadata": {
        "id": "XQEw9avBKoCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReweighedBCLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # YOUR CODE HERE!\n",
        "    pass\n",
        "\n",
        "  def forward(self, batch_predictions, batch_targets, batch_weights) -> torch.Tensor:\n",
        "    # YOUR CODE HERE!\n",
        "    pass"
      ],
      "metadata": {
        "id": "jm2LYrx9RA6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_policy_by_filtered_bc(policy, dataset, weights, n_steps, batch_size) -> DiscretePolicy:\n",
        "  \"\"\"Trains the provided policy by filtered behavioral cloning, by taking n_steps training steps with the\n",
        "  provided optimizer with the provided weights. During training, training batches of size batch_size are sampled from the dataset\n",
        "  to compute the loss.\n",
        "\n",
        "  Args:\n",
        "    policy: policy of class DiscretePolicy.\n",
        "    dataset: The dataset, represented as ____COMPLETE THIS!!!!!!!____.\n",
        "    weights: Weights used in the filtered BC loss.\n",
        "    optimizer: An instance of torch.optim.Optimizer.\n",
        "    n_steps: Number of training steps to take.\n",
        "    batch_size: Size of the sampled batch for each training step.\n",
        "\n",
        "  Returns:\n",
        "    A policy trained according to the parameters above.\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE!\n",
        "  pass"
      ],
      "metadata": {
        "id": "TjRrH-RNTgYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Filtering Strategy I: Trajectory-Level Reweighing???\n",
        "\n",
        "For this problem, we will try a simple strategy: all of the $(s, a)$ pairs obtained from one trajectory will have the same weight, determined by a function of the trajectory return. To formally define this weighing scheme, let $\\tau_1, \\tau_2, \\dots, \\tau_M$ denote the trajectories found in the dataset. Then, for any $(s, a)$ contained in $\\tau_i$, the corresponding weight $w$ is given by\n",
        "\n",
        "$$w = \\text{Softmax}_i\\left[\\frac{1}{\\alpha}R(\\tau_1)\\ , \\frac{1}{\\alpha}R(\\tau_2)\\ , \\dots\\ , \\frac{1}{\\alpha}R(\\tau_m)\\right],$$\n",
        "\n",
        "where $R(\\tau_i)$ is the discounted return of trajectory $i$ and $\\alpha > 0$ is a hyperparameter referred to as the _temperature_."
      ],
      "metadata": {
        "id": "ZNskDXL_UfM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement the function which computes the weights as defined above in the cell below, and performed filtered behavioral cloning with the computed weights.** Feel free to tune the temperature as necessary. How does the trained policy compare to that of the earlier policies in Problems 1(d) and 2(a)?\n",
        "\n",
        "Hint: Remember that the dataset includes an array `is_rollout_start` indicating whether each datapoint is the start of a new trajectory."
      ],
      "metadata": {
        "id": "eNKCudD6Em00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "def trajectory_level_return_weights(dataset, temp, discount) -> np.ndarray:\n",
        "  \"\"\"Computes an array of weights for each point in the provided dataset according to the trajectory-level reweighing scheme.\n",
        "\n",
        "  Args:\n",
        "    dataset: Input dataset.\n",
        "    temp: Temperature used in softmax.\n",
        "    discount: Discount used to compute return.\n",
        "\n",
        "  Returns:\n",
        "    An array of weights for each BC datapoint in the dataset.\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE!\n",
        "  pass"
      ],
      "metadata": {
        "id": "ApVKsC9w4ezF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### RUN FILTERED BC HERE!\n",
        "pass"
      ],
      "metadata": {
        "id": "sjxtmbjeICST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: Why does the choice of weighing function work here? How does it affect what the behavioral cloning loss is doing?**"
      ],
      "metadata": {
        "id": "YLygI3gm3X21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠"
      ],
      "metadata": {
        "id": "ih-skNSV3nx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: What is the effect of the temperature on the weighing scheme?**\n",
        "\n",
        "Hint: As a starting point, think about what happens to the softmax function as $\\alpha \\to 0$ (to make it even easier to think about, consider applying the softmax to two fixed values $a, b$ with $a > b$ as you do this)."
      ],
      "metadata": {
        "id": "G93L_6PM4KBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠"
      ],
      "metadata": {
        "id": "8pDPvdPV4Q-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: One could consider a version of this reweighing scheme where we remove the softmax and simply define the weight for $\\tau_i$ as $R(\\tau_i)$. What are the potential pitfalls of such a scheme?**\n",
        "\n",
        "Hint: Think about the values the reward function could take in all kinds of environments."
      ],
      "metadata": {
        "id": "PZys54zQKtyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠"
      ],
      "metadata": {
        "id": "YNv-fcldLcjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Filtering Strategy II: Truncated Future Return???\n",
        "\n",
        "Another spark of insight just hit Flappy Jr.: rather than just defining weights at the level of trajectories, he can also weight each datapoint separately! Floppy's flying motions aren't that sloppy all the time after all, just like how a broken clock is correct twice a day. For any datapoint $(s, a)$ and its reward $r_0$, he looks at the reward $(r_1, \\dots, r_{T - 1})$ of the $T - 1$ following timesteps in the same trajectory\\* and computes\n",
        "\n",
        "$$R_{T}(s, a) := \\sum_{t = 0}^{T - 1}\\gamma^tr_t.$$\n",
        "\n",
        "Then, if the computed values are $R_1, \\dots, R_N$ for all points in the dataset, then the weight $w_i$ of the $i^{\\text{th}}$ point is given by\n",
        "\n",
        "$$w_i = N\\cdot \\text{Softmax}_i\\left[\\frac{1}{\\alpha}R_1\\ , \\frac{1}{\\alpha}R_2\\ , \\dots\\ , \\frac{1}{\\alpha}R_N\\right],$$\n",
        "\n",
        "where $\\alpha$ is the temperature hyperparameter (as in the previous part). Note that there are now two hyperparameters, the truncation horizon $T$, and the temperature $\\alpha$.\n",
        "\n",
        "\\* **If there are fewer than $T - 1$ timesteps after $(s, a)$ in the trajectory that $(s,a)$ belongs to, the remaining timestep rewards in the sum are set to $0$.**"
      ],
      "metadata": {
        "id": "IiSPXTGq5wnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement the function which computes the weights as defined above in the cell below, and performed filtered behavioral cloning with the computed weights.** Feel free to tune the temperature as necessary. How does the trained policy compare to that of the earlier policies in Problems 1(d) and 2(a)?"
      ],
      "metadata": {
        "id": "Q3c8EbBlHJG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "def truncated_future_return_weights(dataset, truncation_horizon, temp, discount) -> np.ndarray:\n",
        "  \"\"\"Computes an array of weights for each point in the provided dataset according to the truncated future return reweighing scheme.\n",
        "\n",
        "  Args:\n",
        "    dataset: Input dataset.\n",
        "    truncation_horizon: How many timesteps to consider for computing future return.\n",
        "    temp: Temperature used in softmax.\n",
        "    discount: Discount used to compute return.\n",
        "\n",
        "  Returns:\n",
        "    An array of weights for each BC datapoint in the dataset.\n",
        "  \"\"\"\n",
        "  pass"
      ],
      "metadata": {
        "id": "YiKkaRVNHHe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE: RUN FILTERED BC BELOW!\n",
        "pass"
      ],
      "metadata": {
        "id": "socBfHCRIy7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: Let us explore the effect of the hyperparameter $T$ on the weighing scheme. Give a succinct description of the weights when $T = 1$. Do you expect this to work well in general? Why or why not?**"
      ],
      "metadata": {
        "id": "gI2hxo6lIfsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠"
      ],
      "metadata": {
        "id": "dTqP8VprJbGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: Can you think of a reason why one can set $T$ relatively small in Flappy Bird and still obtain decent performance?**\n",
        "\n",
        "Hint: Think about the structure of the problem the agent is trying to solve."
      ],
      "metadata": {
        "id": "lAacgrVZJufo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠"
      ],
      "metadata": {
        "id": "v11Pva51Jsh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e) Optional: Other Reweighing Schemes???\n",
        "\n",
        "Feel free to try implementing other reweighing schemes here! Options to try:\n",
        "\n",
        "1. Only training on the top $q\\%$ of trajectories ranked by return.\n",
        "2. Using the future return within the trajectory from the current timestep."
      ],
      "metadata": {
        "id": "uv0yRwQiOTmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_reweighing_scheme(dataset, *args, **kwargs) -> np.ndarray:\n",
        "  \"\"\"Computes an array of weights for each point in the provided dataset according to the truncated future return reweighing scheme.\n",
        "\n",
        "  Args:\n",
        "    dataset: Input dataset.\n",
        "    *args: Replace with your scheme's hyperparameters.\n",
        "    **kwargs: Replace with your scheme's hyperparameters.\n",
        "\n",
        "  Returns:\n",
        "    An array of weights for each BC datapoint in the dataset.\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE!\n",
        "  pass\n",
        "\n",
        "\n",
        "### YOUR CODE HERE: RUN FILTERED BC BELOW!"
      ],
      "metadata": {
        "id": "4dGOfDWePWy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}